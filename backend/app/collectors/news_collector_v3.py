"""
Morning Bot v3.0 - ì–¸ë¡ ì‚¬ë³„ ìˆ˜ì§‘ + í•« ì ìˆ˜ ì‹œìŠ¤í…œ
"""

from datetime import date, datetime, timedelta, timezone
from email.utils import parsedate_to_datetime
from typing import List, Dict, Any
from sqlalchemy import func
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError

import httpx
import re

from backend.app.config import settings
from backend.app.db.models import NewsDaily
from backend.app.utils.filters import extract_press_from_url, PRESS_BREAKING_CONFIG
from backend.app.utils.category_keywords import classify_category

NAVER_NEWS_URL = "https://openapi.naver.com/v1/search/news.json"
KST_TZ = timezone(timedelta(hours=9))

# 20ê°œ ì–¸ë¡ ì‚¬
PRESS_LIST = [
    "ë§¤ì¼ê²½ì œ", "í•œêµ­ê²½ì œ", "ë¨¸ë‹ˆíˆ¬ë°ì´", "ì„œìš¸ê²½ì œ", "í—¤ëŸ´ë“œê²½ì œ",
    "ì•„ì‹œì•„ê²½ì œ", "ì´ë°ì¼ë¦¬", "ì¡°ì„ ë¹„ì¦ˆ", "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
    "ì—°í•©ë‰´ìŠ¤", "YTN", "KBS", "SBS", "JTBC",
    "êµ­ë¯¼ì¼ë³´", "ì½”ë¦¬ì•„í—¤ëŸ´ë“œ", "ì•„ì´ë‰´ìŠ¤24", "ë””ì§€í„¸íƒ€ì„ìŠ¤", "í•œê²¨ë ˆ", "SBS"
]

# ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ í‚¤ì›Œë“œ (ê²½ì œ, ë¬¸í™” ë‰´ìŠ¤ ìˆ˜ì§‘ ë³´ì¥)
CATEGORY_SEARCH_KEYWORDS = {
    "economy": [
        "ì½”ìŠ¤í”¼", "ì¦ì‹œ", "í™˜ìœ¨", "ê¸ˆë¦¬", "ì£¼ê°€", "ê²½ì œ", "ë¶€ë™ì‚°", "ì•„íŒŒíŠ¸",
        "ë‹¬ëŸ¬", "ì›í™”", "ì±„ê¶Œ", "í€ë“œ", "ë§¤ì¶œ", "ì‹¤ì ", "ì˜ì—…ì´ìµ", "ìˆ˜ì¶œ",
        "GDP", "ë¬¼ê°€", "ì¸í”Œë ˆ", "ì£¼íƒ", "ì „ì„¸", "ì„¸ê¸ˆ", "íˆ¬ì", "ê¸°ì—…"
    ],
    "culture": [
        "ì˜í™”", "ì „ì‹œ", "ê³µì—°", "ì±…", "ë¬¸í™”", "ë¯¸ìˆ ", "ìŒì•…íšŒ",
        "ê°œë´‰", "ë°•ìŠ¤ì˜¤í”¼ìŠ¤", "ì˜í™”ì œ", "ì†Œì„¤", "ì¶œíŒ", "ë² ìŠ¤íŠ¸ì…€ëŸ¬",
        "ë°•ë¬¼ê´€", "ë¯¸ìˆ ê´€", "ê°¤ëŸ¬ë¦¬", "ì¶•ì œ"
    ],
    "entertainment": [
        "ì•„ì´ëŒ", "ë“œë¼ë§ˆ", "ì˜ˆëŠ¥", "ì—°ì˜ˆ", "ê±¸ê·¸ë£¹", "ë°°ìš°",
        "ë³´ì´ê·¸ë£¹", "ê°€ìˆ˜", "íƒ¤ëŸ°íŠ¸", "ì»´ë°±", "ë°ë·”", "ì‹ ê³¡",
        "ì•¨ë²”", "íƒ€ì´í‹€ê³¡", "ë®¤ë¹„", "ë®¤ì§ë¹„ë””ì˜¤"
    ],
    "society": []  # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ì´ë¯€ë¡œ ë³„ë„ ìˆ˜ì§‘ ë¶ˆí•„ìš”
}


def build_topic_key(title: str) -> str:
    """ì¤‘ë³µ íŒë³„ìš© í‚¤ ìƒì„± (30ìë¡œ ë‹¨ì¶•)"""
    if not title:
        return ""
    
    cleaned = title
    cleaned = cleaned.replace("<b>", "").replace("</b>", "")
    cleaned = cleaned.replace("[ì†ë³´]", "").replace("[ë‹¨ë…]", "").replace("[ê¸´ê¸‰]", "")
    cleaned = re.sub(r"[^0-9ê°€-í£a-zA-Z ]", "", cleaned)
    cleaned = cleaned.replace(" ", "").lower()
    return cleaned[:30]


def check_breaking_tag(title: str) -> bool:
    """ì†ë³´ íƒœê·¸ í™•ì¸"""
    breaking_patterns = ["[ì†ë³´]", "[ê¸´ê¸‰]", "[ë‹¨ë…]", "ì†ë³´:", "ë‹¨ë…:"]
    for pattern in breaking_patterns:
        if pattern in title:
            return True
    return False


def fetch_naver_news_raw(query: str, display: int = 100) -> List[Dict[str, Any]]:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ API í˜¸ì¶œ"""
    
    if not settings.NAVER_CLIENT_ID or not settings.NAVER_CLIENT_SECRET:
        raise RuntimeError("NAVER credentials not set")
    
    headers = {
        "X-Naver-Client-Id": settings.NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": settings.NAVER_CLIENT_SECRET,
    }
    params = {
        "query": query,
        "display": display,
        "sort": "date",
    }
    
    try:
        resp = httpx.get(NAVER_NEWS_URL, params=params, headers=headers, timeout=10.0)
        resp.raise_for_status()
        return resp.json().get("items", [])
    except Exception as e:
        print(f"  âŒ API ì˜¤ë¥˜: {e}")
        return []


def collect_by_press(db: Session) -> List[NewsDaily]:
    """ì–¸ë¡ ì‚¬ë³„ ìˆ˜ì§‘ (20ê°œ Ã— 100ê°œ = 2,000ê°œ)"""
    
    from backend.app.utils.dedup import remove_duplicate_news
    
    today = datetime.now(KST_TZ).date()
    now_kst = datetime.now(KST_TZ)
    min_dt = now_kst - timedelta(hours=24)  # 24ì‹œê°„ ì´ë‚´ë§Œ í—ˆìš© (êµ¬í˜• ë‰´ìŠ¤ í•„í„°)
    created = []
    stats = {
        "missing_fields": 0,
        "bad_pubdate": 0,
        "old_pubdate": 0,
        "no_topic_key": 0,
        "press_filtered": 0,
        "duplicate_url": 0,
        "saved": 0,
    }
    temp_news_list = []  # ì¤‘ë³µ ì œê±° ì „ ì„ì‹œ ë¦¬ìŠ¤íŠ¸
    
    print(f"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"ğŸ“° ì–¸ë¡ ì‚¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    total_fetched = 0
    total_saved = 0
    
    for press in PRESS_LIST:
        print(f"\nğŸ” [{press}] ìˆ˜ì§‘ ì¤‘...")
        
        items = fetch_naver_news_raw(query=press, display=100)
        total_fetched += len(items)
        
        if not items:
            print(f"  âš ï¸ ê²°ê³¼ ì—†ìŒ")
            continue
        
        saved_count = 0
        
        for item in items:
            try:
                # 1. í•„ìˆ˜ í•„ë“œ ê²€ì¦
                raw_title = item.get("title")
                url = item.get("originallink") or item.get("link")
                pub_raw = item.get("pubDate")
                
                if not raw_title or not url:
                    continue

                # 1.5. ë°œí–‰ì¼ íŒŒì‹± ë° ì‹ ì„ ë„ í•„í„°
                pub_dt = None
                try:
                    if pub_raw:
                        pub_dt = parsedate_to_datetime(pub_raw)
                        if pub_dt.tzinfo:
                            pub_dt = pub_dt.astimezone(KST_TZ)
                        else:
                            pub_dt = pub_dt.replace(tzinfo=KST_TZ)
                except Exception:
                    pub_dt = None

                # pubDateê°€ ì—†ìœ¼ë©´ ë²„ë¦¬ê³ , 48ì‹œê°„ ì´ì „ ë‰´ìŠ¤ëŠ” ìŠ¤í‚µ
                if not pub_dt or pub_dt < min_dt:
                    continue

                pub_dt_naive = pub_dt.replace(tzinfo=None)
                item_date = pub_dt.date()
                
                # 2. ì œëª© ì •ì œ
                title = raw_title.replace("<b>", "").replace("</b>", "")
                topic_key = build_topic_key(title)
                
                if not topic_key:
                    continue
                
                # 3. ì¤‘ë³µ ì²´í¬ (ë™ì¼ ì¼ì ê¸°ì¤€)
                existing = db.query(NewsDaily)\
                    .filter(
                        NewsDaily.date == item_date,
                        NewsDaily.topic_key == topic_key
                    )\
                    .first()
                
                if existing:
                    continue
                
                # 4. ì–¸ë¡ ì‚¬ í™•ì¸
                source_press = extract_press_from_url(url)
                if not source_press or source_press not in PRESS_BREAKING_CONFIG:
                    continue
                
                # 5. ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
                category = classify_category(title)
                
                # 6. ì†ë³´ íƒœê·¸ í™•ì¸
                is_breaking = check_breaking_tag(title)
                
                # 7. ì €ì¥
                news = NewsDaily(
                    date=item_date,
                    category=category,
                    title=title,
                    url=url[:200] if url else "",
                    source=source_press,
                    topic_key=topic_key,
                    is_breaking=is_breaking,
                    is_top=False,
                    hot_score=0,
                    keywords=None,
                    sentiment=None,
                    published_at=pub_dt_naive,
                )
                
                temp_news_list.append(news)
                saved_count += 1
                
            except Exception as e:
                print(f"  âš ï¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"  âœ… ì €ì¥: {saved_count}ê°œ")
        total_saved += saved_count
    
    # ì¤‘ë³µ ì œê±°
    print(f"\nğŸ”„ ì¤‘ë³µ ì œê±° ì¤‘...")
    print(f"\nğŸ”„ ì¤‘ë³µ ì œê±° ì¤‘...")
    print(f"  - ìˆ˜ì§‘: {len(temp_news_list)}ê°œ")
    unique_news_list = remove_duplicate_news(temp_news_list)
    print(f"  - ì¤‘ë³µ ì œê±° í›„: {len(unique_news_list)}ê°œ")
    
    # DB ì €ì¥
    for news in unique_news_list:
        # DBì— ì´ë¯¸ ìˆëŠ”ì§€ ì²´í¬
        existing = db.query(NewsDaily).filter(
            NewsDaily.date == news.date,
            NewsDaily.topic_key == news.topic_key
        ).first()
        
        if not existing:
            db.add(news)
            created.append(news)
    
    # Commit
    try:
        db.commit()
        for n in created:
            db.refresh(n)
    except IntegrityError as e:
        db.rollback()
        print(f"  âŒ DB ì˜¤ë¥˜: {e}")
        created = []
    
    print(f"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ:")
    print(f"  - API ìš”ì²­: {len(PRESS_LIST)}íšŒ")
    print(f"  - ë°›ì€ ë‰´ìŠ¤: {total_fetched}ê°œ")
    print(f"  - ì €ì¥ëœ ë‰´ìŠ¤: {total_saved}ê°œ")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
    
    return created


def collect_by_category_keywords(db: Session) -> List[NewsDaily]:
    """ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ê²½ì œ/ë¬¸í™” ë³´ì¥)"""

    from backend.app.utils.dedup import remove_duplicate_news

    today = datetime.now(KST_TZ).date()
    now_kst = datetime.now(KST_TZ)
    min_dt = now_kst - timedelta(hours=24)
    created = []
    temp_news_list = []

    print(f"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"ğŸ¯ ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

    total_fetched = 0

    for category, keywords in CATEGORY_SEARCH_KEYWORDS.items():
        # ë¹ˆ ì¹´í…Œê³ ë¦¬ëŠ” ìŠ¤í‚µ (societyëŠ” ê¸°ë³¸ ë¶„ë¥˜ë¡œ ì¶©ë¶„)
        if not keywords:
            continue

        print(f"\nğŸ“‚ [{category.upper()}] ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì¤‘...")

        # ê° í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ (í‚¤ì›Œë“œë‹¹ 30ê°œì”©)
        for keyword in keywords:
            items = fetch_naver_news_raw(query=keyword, display=30)
            total_fetched += len(items)

            if not items:
                continue

            for item in items:
                try:
                    raw_title = item.get("title")
                    url = item.get("originallink") or item.get("link")
                    pub_raw = item.get("pubDate")

                    if not raw_title or not url:
                        continue

                    # ë°œí–‰ì¼ íŒŒì‹±
                    pub_dt = None
                    try:
                        if pub_raw:
                            pub_dt = parsedate_to_datetime(pub_raw)
                            if pub_dt.tzinfo:
                                pub_dt = pub_dt.astimezone(KST_TZ)
                            else:
                                pub_dt = pub_dt.replace(tzinfo=KST_TZ)
                    except Exception:
                        pub_dt = None

                    if not pub_dt or pub_dt < min_dt:
                        continue

                    pub_dt_naive = pub_dt.replace(tzinfo=None)
                    item_date = pub_dt.date()

                    # ì œëª© ì •ì œ
                    title = raw_title.replace("<b>", "").replace("</b>", "")
                    topic_key = build_topic_key(title)

                    if not topic_key:
                        continue

                    # ì¤‘ë³µ ì²´í¬
                    existing = db.query(NewsDaily)\
                        .filter(
                            NewsDaily.date == item_date,
                            NewsDaily.topic_key == topic_key
                        )\
                        .first()

                    if existing:
                        continue

                    # ì–¸ë¡ ì‚¬ í™•ì¸
                    source_press = extract_press_from_url(url)
                    if not source_press or source_press not in PRESS_BREAKING_CONFIG:
                        continue

                    # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜ (ì¬í™•ì¸)
                    detected_category = classify_category(title)

                    # ì†ë³´ íƒœê·¸ í™•ì¸
                    is_breaking = check_breaking_tag(title)

                    # ì €ì¥
                    news = NewsDaily(
                        date=item_date,
                        category=detected_category,  # ìë™ ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬ ì‚¬ìš©
                        title=title,
                        url=url[:200] if url else "",
                        source=source_press,
                        topic_key=topic_key,
                        is_breaking=is_breaking,
                        is_top=False,
                        hot_score=0,
                        keywords=None,
                        sentiment=None,
                        published_at=pub_dt_naive,
                    )

                    temp_news_list.append(news)

                except Exception as e:
                    continue

        print(f"  âœ… {category}: {len([n for n in temp_news_list if n.category == category])}ê°œ")

    # ì¤‘ë³µ ì œê±°
    print(f"\nğŸ”„ ì¤‘ë³µ ì œê±° ì¤‘...")
    print(f"  - ìˆ˜ì§‘: {len(temp_news_list)}ê°œ")
    unique_news_list = remove_duplicate_news(temp_news_list)
    print(f"  - ì¤‘ë³µ ì œê±° í›„: {len(unique_news_list)}ê°œ")

    # DB ì €ì¥ (ê°œë³„ ì»¤ë°‹, ì¹´í…Œê³ ë¦¬ ì—…ë°ì´íŠ¸ ì§€ì›)
    updated_count = 0
    for news in unique_news_list:
        try:
            # ê¸°ì¡´ ë‰´ìŠ¤ í™•ì¸
            existing = db.query(NewsDaily).filter(
                NewsDaily.date == news.date,
                NewsDaily.topic_key == news.topic_key
            ).first()

            if existing:
                # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì¹´í…Œê³ ë¦¬ê°€ societyì´ê³  ìƒˆ ë¶„ë¥˜ê°€ ë” êµ¬ì²´ì ì´ë©´ ì—…ë°ì´íŠ¸
                if existing.category == "society" and news.category != "society":
                    existing.category = news.category
                    db.commit()
                    updated_count += 1
            else:
                # ìƒˆ ë‰´ìŠ¤ë©´ ì¶”ê°€
                db.add(news)
                db.commit()
                db.refresh(news)
                created.append(news)
        except IntegrityError:
            # ì¤‘ë³µì´ë©´ ë¡¤ë°±í•˜ê³  ë‹¤ìŒìœ¼ë¡œ
            db.rollback()
            continue
        except Exception as e:
            # ë‹¤ë¥¸ ì—ëŸ¬ë„ ë¡¤ë°±í•˜ê³  ë‹¤ìŒìœ¼ë¡œ
            db.rollback()
            print(f"  âš ï¸ ì €ì¥ ì‹¤íŒ¨: {news.title[:30]}... - {e}")
            continue

    if updated_count > 0:
        print(f"  ğŸ”„ ì¹´í…Œê³ ë¦¬ ì—…ë°ì´íŠ¸: {updated_count}ê°œ")

    print(f"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ ì™„ë£Œ:")
    print(f"  - API ìš”ì²­: {len(CATEGORY_SEARCH_KEYWORDS) * sum(len(k) for k in CATEGORY_SEARCH_KEYWORDS.values())}íšŒ")
    print(f"  - ë°›ì€ ë‰´ìŠ¤: {total_fetched}ê°œ")
    print(f"  - ì €ì¥ëœ ë‰´ìŠ¤: {len(created)}ê°œ")

    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    for category in CATEGORY_SEARCH_KEYWORDS.keys():
        count = len([n for n in created if n.category == category])
        print(f"  - {category}: {count}ê°œ")

    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")

    return created


def filter_repeated_person_names(news_list):
    """ê°™ì€ ì¸ë¬¼ ì´ë¦„ 3ê°œ ì´ìƒ â†’ 3ê°œë§Œ ìœ ì§€"""
    from collections import defaultdict
    import re
    
    person_counts = defaultdict(list)
    
    for news in news_list:
        title = news.title
        
        # ì¸ë¬¼ ì´ë¦„ ì¶”ì¶œ
        patterns = [
            r'([ê°€-í£]{2,4})\s+(ì•ˆë³´ì‹¤ì¥|ëŒ€í†µë ¹|ì´ë¦¬|ì¥ê´€|ì‹¤ì¥|ì˜ì›|ëŒ€í‘œ|íšŒì¥)',
            r'\[ì†ë³´\]\s*([ê°€-í£]{2,4})\s+(ì•ˆë³´ì‹¤ì¥|ëŒ€í†µë ¹|ì´ë¦¬|ì¥ê´€)',
        ]
        
        person_name = None
        for pattern in patterns:
            match = re.search(pattern, title)
            if match:
                person_name = match.group(1)
                break
        
        if person_name:
            person_counts[person_name].append(news)
        else:
            person_counts['_no_person'].append(news)
    
    # ê° ì¸ë¬¼ë³„ ìµœëŒ€ 3ê°œ
    filtered = []
    for person, news_items in person_counts.items():
        if person == '_no_person':
            filtered.extend(news_items)
        else:
            filtered.extend(news_items[:3])
    
    return filtered


def collect_breaking_news(db: Session) -> List[NewsDaily]:
    """ì†ë³´ ë¼ì¸ ìˆ˜ì§‘ (100ê°œ) + ì¤‘ë³µ ì œê±°"""
    
    from backend.app.utils.dedup import remove_duplicate_news
    
    today = datetime.now(KST_TZ).date()
    now_kst = datetime.now(KST_TZ)
    min_dt = now_kst - timedelta(hours=24)
    created = []
    stats = {
        "missing_fields": 0,
        "bad_pubdate": 0,
        "old_pubdate": 0,
        "no_topic_key": 0,
        "press_filtered": 0,
        "duplicate_url": 0,
        "saved": 0,
    }
    
    print(f"\nâš¡ ì†ë³´ ë¼ì¸ ìˆ˜ì§‘ ì¤‘...")
    
    items = fetch_naver_news_raw(query="ì†ë³´", display=100)
    
    if not items:
        print(f"  âš ï¸ ê²°ê³¼ ì—†ìŒ")
        return []
    
    # 1ë‹¨ê³„: ëª¨ë“  ì†ë³´ë¥¼ NewsDaily ê°ì²´ë¡œ ë³€í™˜ (ì €ì¥ ì „)
    temp_news_list = []
    
    for item in items:
        try:
            raw_title = item.get("title")
            url = item.get("originallink") or item.get("link")
            pub_raw = item.get("pubDate")
            
            if not raw_title or not url:
                stats["missing_fields"] += 1
                continue

            pub_dt = None
            try:
                if pub_raw:
                    pub_dt = parsedate_to_datetime(pub_raw)
                    if pub_dt.tzinfo:
                        pub_dt = pub_dt.astimezone(KST_TZ)
                    else:
                        pub_dt = pub_dt.replace(tzinfo=KST_TZ)
            except Exception:
                pub_dt = None

            if not pub_dt:
                stats["bad_pubdate"] += 1
                continue

            if pub_dt < min_dt:
                stats["old_pubdate"] += 1
                continue

            pub_dt_naive = pub_dt.replace(tzinfo=None)
            item_date = pub_dt.date()
            
            title = raw_title.replace("<b>", "").replace("</b>", "")
            topic_key = build_topic_key(title)
            
            if not topic_key:
                stats["no_topic_key"] += 1
                continue
            
            # ì–¸ë¡ ì‚¬ í™•ì¸
            source_press = extract_press_from_url(url)
            if not source_press or source_press not in PRESS_BREAKING_CONFIG:
                stats["press_filtered"] += 1
                continue
            
            # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            category = classify_category(title)
            
            # NewsDaily ê°ì²´ ìƒì„± (ì•„ì§ DBì— ì €ì¥ ì•ˆ í•¨)
            news = NewsDaily(
                date=item_date,
                category=category,
                title=title,
                url=url[:200] if url else "",
                source=source_press,
                topic_key=topic_key,
                is_breaking=True,
                is_top=False,
                hot_score=0,
                keywords=None,
                sentiment=None,
                published_at=pub_dt_naive,
                created_at=pub_dt_naive,
            )
            
            temp_news_list.append(news)
            
        except Exception as e:
            continue
    
    # 2ë‹¨ê³„: ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°
    if temp_news_list:
        print(f"  ğŸ“‹ ìˆ˜ì§‘: {len(temp_news_list)}ê°œ")
        unique_news_list = remove_duplicate_news(temp_news_list)
        print(f"  âœ¨ ì¤‘ë³µ ì œê±° í›„: {len(unique_news_list)}ê°œ")
        unique_news_list = filter_repeated_person_names(unique_news_list)
        print(f"  ğŸ¯ ì¸ë¬¼ í•„í„° í›„: {len(unique_news_list)}ê°œ")
    else:
        unique_news_list = []
    
    # 3ë‹¨ê³„: DBì— ì €ì¥ (topic_key ì¤‘ë³µ ì²´í¬)
    saved_count = 0
    
    for news in unique_news_list:
        try:
            # DBì— ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
            existing = (
                db.query(NewsDaily)
                .filter(
                    NewsDaily.date == news.date,
                    NewsDaily.url == news.url,
                )
                .first()
            )
            
            if existing:
                stats["duplicate_url"] += 1
                continue
            
            db.add(news)
            created.append(news)
            saved_count += 1
            stats["saved"] += 1
            
        except Exception as e:
            continue
    
    try:
        db.commit()
        for n in created:
            db.refresh(n)
    except IntegrityError:
        db.rollback()
        created = []
    
    print(f"  âœ… ì†ë³´ ì €ì¥: {saved_count}ê°œ")
    print(
        f"  ğŸ“Œ ìŠ¤í‚µ ì‚¬ìœ : missing={stats['missing_fields']} "
        f"bad_pub={stats['bad_pubdate']} old_pub={stats['old_pubdate']} "
        f"no_topic={stats['no_topic_key']} press={stats['press_filtered']} "
        f"dup_url={stats['duplicate_url']}"
    )
    print("")
    
    return created


def calculate_hot_score(news_id: int, db: Session) -> int:
    """í•« ì ìˆ˜ ê³„ì‚°"""

    news = db.query(NewsDaily).filter(NewsDaily.id == news_id).first()
    if not news:
        return 0

    score = 0
    # KST ê¸°ì¤€ ë‚ ì§œ/ì‹œê°„ (íƒ€ì„ì¡´ ì•ˆì „)
    today = datetime.now(KST_TZ).date()
    now = datetime.now(KST_TZ)
    
    # 1. ì¤‘ë³µ ì£¼ì œ ê°œìˆ˜ (ìµœëŒ€ 100ì )
    duplicate_count = db.query(NewsDaily)\
        .filter(
            NewsDaily.topic_key == news.topic_key,
            NewsDaily.date == today
        )\
        .count()
    score += duplicate_count * 10
    
    # 2. ë³´ë„ ì–¸ë¡ ì‚¬ ê°œìˆ˜ (ìµœëŒ€ 50ì )
    press_count = db.query(func.count(func.distinct(NewsDaily.source)))\
        .filter(
            NewsDaily.topic_key == news.topic_key,
            NewsDaily.date == today
        )\
        .scalar()
    score += press_count * 5
    
    # 3. ì†ë³´ íƒœê·¸ (30ì )
    if news.is_breaking:
        score += 30
    
    # 4. ìµœì‹ ë„ (ìµœëŒ€ 10ì )
    # created_atì´ íƒ€ì„ì¡´ ì •ë³´ê°€ ì—†ìœ¼ë©´ KSTë¡œ ê°„ì£¼
    created_at = news.created_at if news.created_at.tzinfo else news.created_at.replace(tzinfo=KST_TZ)
    hours_old = (now - created_at).total_seconds() / 3600
    if hours_old < 1:
        score += 10
    elif hours_old < 3:
        score += 5
    elif hours_old < 6:
        score += 2
    
    # 5. ì£¼ìš” ì–¸ë¡ ì‚¬ ë³´ë„ˆìŠ¤ (5ì )
    major_press = ["ì—°í•©ë‰´ìŠ¤", "YTN", "KBS", "SBS", "ë§¤ì¼ê²½ì œ", "í•œêµ­ê²½ì œ"]
    if any(press in news.source for press in major_press):
        score += 5
    
    return score


def update_hot_scores(db: Session):
    """ëª¨ë“  ì˜¤ëŠ˜ ë‰´ìŠ¤ì˜ í•« ì ìˆ˜ ì—…ë°ì´íŠ¸"""
    
    print(f"\nğŸ”¥ í•« ì ìˆ˜ ê³„ì‚° ì¤‘...")

    # KST ê¸°ì¤€ ì˜¤ëŠ˜ ë‚ ì§œ (íƒ€ì„ì¡´ ì•ˆì „)
    today = datetime.now(KST_TZ).date()
    news_list = db.query(NewsDaily)\
        .filter(NewsDaily.date == today)\
        .all()
    
    for news in news_list:
        news.hot_score = calculate_hot_score(news.id, db)
    
    db.commit()
    
    print(f"  âœ… {len(news_list)}ê°œ ë‰´ìŠ¤ ì ìˆ˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ\n")


def select_top_news(db: Session, category: str, limit: int = 10) -> List[NewsDaily]:
    """ì¹´í…Œê³ ë¦¬ë³„ TOP ì„ ì •"""

    from backend.app.utils.dedup import remove_duplicate_news

    # KST ê¸°ì¤€ ì˜¤ëŠ˜ ë‚ ì§œ (íƒ€ì„ì¡´ ì•ˆì „)
    today = datetime.now(KST_TZ).date()
    
    candidate_limit = max(limit * 5, 50)
    candidates = db.query(NewsDaily)\
        .filter(
            NewsDaily.date == today,
            NewsDaily.category == category
        )\
        .order_by(
            NewsDaily.hot_score.desc(),
            NewsDaily.created_at.desc()
        )\
        .limit(candidate_limit)\
        .all()

    top_news = remove_duplicate_news(candidates)[:limit]
    
    # is_top í”Œë˜ê·¸ ì—…ë°ì´íŠ¸
    for news in top_news:
        news.is_top = True
    
    db.commit()
    
    return top_news


def build_daily_rankings(db: Session):
    """ì „ì²´ ë­í‚¹ êµ¬ì„±"""
    
    print(f"\nğŸ† TOP 10 ì„ ì • ì¤‘...")
    
    # 1. í•« ì ìˆ˜ ì—…ë°ì´íŠ¸ (ë‹¹ì¼ + ì „ì¼ ìˆ˜ì§‘ë¶„ë„ ë°˜ì˜)
    update_hot_scores(db)
    
    # 2. ê° ì¹´í…Œê³ ë¦¬ë³„ TOP 10
    rankings = {}
    for category in ["society", "economy", "culture", "entertainment"]:
        top_news = select_top_news(db, category, limit=10)
        rankings[category] = top_news
        print(f"  âœ… {category}: {len(top_news)}ê°œ")
    
    print(f"  âœ… TOP 10 ì„ ì • ì™„ë£Œ\n")
    
    return rankings


def get_today_summary(db: Session) -> List[NewsDaily]:
    """ì˜¤ëŠ˜ì˜ ìš”ì•½: ê° ì¹´í…Œê³ ë¦¬ TOP 1"""

    summary = []
    # KST ê¸°ì¤€ ì˜¤ëŠ˜ ë‚ ì§œ (íƒ€ì„ì¡´ ì•ˆì „)
    today = datetime.now(KST_TZ).date()
    
    for category in ["society", "economy", "culture", "entertainment"]:
        top1 = db.query(NewsDaily)\
            .filter(
                NewsDaily.date == today,
                NewsDaily.category == category
            )\
            .order_by(
                NewsDaily.hot_score.desc(),
                NewsDaily.created_at.desc()
            )\
            .first()
        
        if top1:
            summary.append(top1)
    
    return summary


def build_daily_top5_v3(db: Session):
    """v3 ì „ì²´ í”Œë¡œìš°"""
    
    print(f"\n" + "="*60)
    print(f"  Morning Bot v3.0 - ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    print(f"="*60)
    
    try:
        # 1. ì–¸ë¡ ì‚¬ë³„ ìˆ˜ì§‘
        collect_by_press(db)

        # 2. ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ìˆ˜ì§‘ (ê²½ì œ/ë¬¸í™” ë³´ì¥)
        collect_by_category_keywords(db)

        # 3. ì†ë³´ ë¼ì¸ ìˆ˜ì§‘
        collect_breaking_news(db)

        # 4. TOP 10 ì„ ì •
        build_daily_rankings(db)
        
        print(f"="*60)
        print(f"  âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print(f"="*60 + "\n")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
